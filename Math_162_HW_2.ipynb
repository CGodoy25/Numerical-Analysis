{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFHY8Fdc6i2CGqXBQGYTVa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Objective\n",
        "\n",
        "### Part 1 - Write python functions that solve a linear system using the following methods:\n",
        "* Cholesky decomposition\n",
        "* Guass-Seidal iteration\n",
        "* Gradiet descent with exact line search\n",
        "* Conjugate graidient method\n",
        "\n",
        "### Part 2 - Test each method for $n=10$ and $n=100$. Measure and report CPU time for each method.\n",
        "---"
      ],
      "metadata": {
        "id": "Ybsdo37wpLwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear System\n",
        "\n",
        "The linear system used in this project is as follows:\n",
        "\n",
        "$$Ax=b, A \\in \\mathbb{R}^{n \\times n}, A=A^T$$\n",
        "\n",
        "where the matrix $A$ is defined as:\n",
        "$$A=R^TR+nI, A \\in \\mathbb{R}^{n \\times n}, A=A^T$$\n",
        "\n",
        "where $R \\in \\mathbb{R}^{n \\times n}$ is a random matrix with entries $R_{ij} \\sim \\text{Uniform}(0,1)$, $I$ is the indentity matrix, and $b \\in \\mathbb{R}$ is a random vector with entries $b_i \\sim \\text{Uniform}(0,1)$\n"
      ],
      "metadata": {
        "id": "FNbyogXNuj7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Method 1 - Cholesky Decomposion\n",
        "\n",
        "Cholesky decomposition is a factorization of a positive definite matrix $A$ into the product of a lower triangular matrix $L$ and its transpose $L^T$. Mathematically, if $A$ is a positive definite matrix, then there exists a unique lower triangular matrix $L$ such that:\n",
        "\n",
        "$$A = L L^T$$\n",
        "\n",
        "Here's how the decomposition works step-by-step:\n",
        "\n",
        "1. **Starting with Matrix $A$**:\n",
        "   $$\n",
        "   A =\n",
        "   \\begin{pmatrix}\n",
        "   a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
        "   a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
        "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "   a_{n1} & a_{n2} & \\cdots & a_{nn}\n",
        "   \\end{pmatrix}\n",
        "   $$\n",
        "\n",
        "2. **Form Matrix $L$** such that:\n",
        "   $$\n",
        "   L =\n",
        "   \\begin{pmatrix}\n",
        "   l_{11} & 0      & \\cdots & 0      \\\\\n",
        "   l_{21} & l_{22} & \\cdots & 0      \\\\\n",
        "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "   l_{n1} & l_{n2} & \\cdots & l_{nn}\n",
        "   \\end{pmatrix}\n",
        "   $$\n",
        "\n",
        "3. **Calculate Elements of $L$**:\n",
        "   - **Diagonal Elements**:\n",
        "     $$\n",
        "     l_{ii} = \\sqrt{a_{ii} - \\sum_{k=1}^{i-1} l_{ik}^2}\n",
        "     $$\n",
        "   - **Off-diagonal Elements**:\n",
        "     $$\n",
        "     l_{ij} = \\frac{1}{l_{jj}} \\left(a_{ij} - \\sum_{k=1}^{j-1} l_{ik} l_{jk}\\right) \\quad \\text{for } i > j\n",
        "     $$\n",
        "\n",
        "For example, if $A$ is a 3x3 matrix:\n",
        "$$\n",
        "A =\n",
        "\\begin{pmatrix}\n",
        "a_{11} & a_{12} & a_{13} \\\\\n",
        "a_{21} & a_{22} & a_{23} \\\\\n",
        "a_{31} & a_{32} & a_{33}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Then the elements of $L$ are calculated as follows:\n",
        "$$\n",
        "l_{11} = \\sqrt{a_{11}}\n",
        "$$\n",
        "$$\n",
        "l_{21} = \\frac{a_{21}}{l_{11}}, \\quad l_{22} = \\sqrt{a_{22} - l_{21}^2}\n",
        "$$\n",
        "$$\n",
        "l_{31} = \\frac{a_{31}}{l_{11}}, \\quad l_{32} = \\frac{a_{32} - l_{31}l_{21}}{l_{22}}, \\quad l_{33} = \\sqrt{a_{33} - l_{31}^2 - l_{32}^2}\n",
        "$$\n",
        "\n",
        "Once all elements of $L$ are computed, we have a Cholesky decomposition $A = LL^T$.\n"
      ],
      "metadata": {
        "id": "NpYBlVwzqpyE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8IV_8Y77ovZx"
      },
      "outputs": [],
      "source": [
        "def cholesky_decomposition(A):\n",
        "    n = A.shape[0]\n",
        "    L = np.zeros_like(A)\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1):\n",
        "            sum_k = sum(L[i][k] * L[j][k] for k in range(j))\n",
        "            if i == j:  # Diagonal elements\n",
        "                L[i][j] = np.sqrt(A[i][i] - sum_k)\n",
        "            else:\n",
        "                L[i][j] = (A[i][j] - sum_k) / L[j][j]\n",
        "\n",
        "    return L\n",
        "\n",
        "def solve_cholesky(A, b):\n",
        "    L = cholesky_decomposition(A)\n",
        "\n",
        "    # Forward substitution to solve Ly = b\n",
        "    n = b.size\n",
        "    y = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        y[i] = (b[i] - np.dot(L[i, :i], y[:i])) / L[i, i]\n",
        "\n",
        "    # Backward substitution to solve L^Tx = y\n",
        "    x = np.zeros(n)\n",
        "    for i in range(n-1, -1, -1):\n",
        "        x[i] = (y[i] - np.dot(L[i+1:, i], x[i+1:])) / L[i, i]\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Method 2 - Gauss Seidel\n",
        "\n",
        "The Gauss-Seidel method is an iterative technique for solving a system of linear equations of the form $Ax = b$. It is particularly useful for large, sparse systems where direct methods may be impractical. The method iterates over each equation, using the most recent values of the solution vector $x$ to update the next values.\n",
        "\n",
        "Mathematically, the Gauss-Seidel method can be described as follows:\n",
        "\n",
        "Given a system of linear equations $Ax = b$, where $A$ is a matrix of coefficients, $x$ is the vector of unknowns, and $b$ is the vector of constants, the $i$-th equation can be written as:\n",
        "\n",
        "$$ a_{ii} x_i^{(k+1)} = b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij} x_j^{(k)} $$\n",
        "\n",
        "for $i = 1, 2, \\ldots, n$, where $x_i^{(k+1)}$ is the updated value of $x_i$ at iteration $k+1$, and $x_i^{(k)}$ is the value of $x_i$ at iteration $k$.\n",
        "\n",
        "The iterations continue until the solution converges, i.e., until the changes in the solution vector $x$ are below a specified tolerance.\n",
        "\n"
      ],
      "metadata": {
        "id": "qRqZVvU11vHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gauss_seidel_iteration(A, b, x0, tol=1e-10, max_iterations=1000):\n",
        "    n = len(b)\n",
        "    x = np.copy(x0)\n",
        "\n",
        "    for k in range(max_iterations):\n",
        "        x_new = np.copy(x)\n",
        "\n",
        "        for i in range(n):\n",
        "            sum1 = np.dot(A[i, :i], x_new[:i])\n",
        "            sum2 = np.dot(A[i, i+1:], x[i+1:])\n",
        "            x_new[i] = (b[i] - sum1 - sum2) / A[i, i]\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(x_new - x, ord=np.inf) < tol:\n",
        "            return x_new, k + 1\n",
        "\n",
        "        x = x_new\n",
        "\n",
        "    return x, max_iterations"
      ],
      "metadata": {
        "id": "79MDKADd2sU-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Method 3 - Gradient Descent with Exact Line Search\n",
        "\n",
        "Gradient Descent with exact line search is an iterative optimization algorithm used to minimize a function. For a linear system $Ax = b$, the objective is to minimize the quadratic function:\n",
        "\n",
        "$$ f(x) = \\frac{1}{2} x^T A x - b^T x $$\n",
        "\n",
        "The algorithm updates the solution vector $x$ using the following steps:\n",
        "\n",
        "1. **Compute the Gradient**: Calculate the gradient of the function at the current point $x_k$:\n",
        "   $$ \\nabla f(x_k) = A x_k - b $$\n",
        "\n",
        "2. **Exact Line Search**: Determine the step size $\\alpha_k$ by minimizing the function along the direction of the gradient:\n",
        "   $$ \\alpha_k = \\frac{(A x_k - b)^T (A x_k - b)}{(A (A x_k - b))^T (A x_k - b)} $$\n",
        "\n",
        "3. **Update**: Update the solution vector $x$ using the step size $\\alpha_k$:\n",
        "   $$ x_{k+1} = x_k - \\alpha_k \\nabla f(x_k) $$\n",
        "\n",
        "The iterations continue until the gradient norm $\\| \\nabla f(x_k) \\|$ is below a specified tolerance.\n"
      ],
      "metadata": {
        "id": "80uHHpcn4BHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(A, b, x0, max_iter=1000, tol=1e-10):\n",
        "  x = x0\n",
        "  grad = A @ x - b\n",
        "\n",
        "  for k in range(max_iter):\n",
        "    alpha = (grad.T @ grad) / (grad.T @ A @ grad) # Compute step size using exact line search\n",
        "    alpha = alpha.item() # Convert to scalar\n",
        "\n",
        "    x = x - alpha * grad # Update solution\n",
        "    grad = A @ x - b # Compute new gradient\n",
        "\n",
        "    if np.linalg.norm(grad) < tol: # Check convergence\n",
        "      break\n",
        "\n",
        "  return x, k + 1"
      ],
      "metadata": {
        "id": "DPsh2_P_4v3L"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Method 4 - Conjugate Gradient Method\n",
        "\n",
        "The Conjugate Gradient method is an iterative algorithm for solving systems of linear equations of the form $Ax = b$, where $A$ is a symmetric positive-definite matrix. The algorithm is particularly useful for large, sparse systems.\n",
        "\n",
        "The method starts with an initial guess $x_0$ and iterates to find the solution by following these steps:\n",
        "\n",
        "1. **Initialization**: Set $r_0 = b - Ax_0$ (residual) and $p_0 = r_0$ (search direction).\n",
        "\n",
        "2. **Iterative Steps**:\n",
        "   For $k = 0, 1, 2, \\ldots$, do the following until convergence:\n",
        "   \\begin{align*}\n",
        "   \\alpha_k &= \\frac{r_k^T r_k}{p_k^T A p_k} \\\\\n",
        "   x_{k+1} &= x_k + \\alpha_k p_k \\\\\n",
        "   r_{k+1} &= r_k - \\alpha_k A p_k \\\\\n",
        "   \\beta_k &= \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k} \\\\\n",
        "   p_{k+1} &= r_{k+1} + \\beta_k p_k\n",
        "   \\end{align*}\n",
        "\n",
        "3. **Convergence**: The iterations continue until the norm of the residual $\\|r_k\\|$ is below a specified tolerance.\n"
      ],
      "metadata": {
        "id": "CQs3P8SR5U4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conjugate_gradient(A, b, x0, tol=1e-10, max_iter=1000):\n",
        "  x = x0\n",
        "  r = A @ x - b # Initial residual r0 = A @ x0 - b\n",
        "  p = r.copy() # Initial direction p0 = r0\n",
        "\n",
        "  for k in range(max_iter):\n",
        "    Ap = A @ p\n",
        "    alpha = (r.T @ r) / (p.T @ Ap) # Compute step size\n",
        "    alpha = alpha.item() # Convert to scalar\n",
        "\n",
        "    x = x - alpha * p # Update solution\n",
        "    r_new = r - alpha * Ap # Update residual\n",
        "\n",
        "    # Check for convergence\n",
        "    if np.linalg.norm(r_new) < tol:\n",
        "      break\n",
        "\n",
        "    beta = (r_new.T @ r_new) / (r.T @ r) # Compute conjugate direction coefficient\n",
        "    beta = beta.item() # Convert to scalar\n",
        "\n",
        "    p = r_new + beta * p # Update direction\n",
        "    r = r_new # Update residual\n",
        "\n",
        "    return x, k + 1 # Return solution and number of iterations"
      ],
      "metadata": {
        "id": "BpvsfL665eyc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 2 - Testing Each Method"
      ],
      "metadata": {
        "id": "3lFKCWHpiSD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the case where $n=10$"
      ],
      "metadata": {
        "id": "HJu51ORxOZVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "n = 10\n",
        "A = np.random.rand(n, n)\n",
        "A = np.matmul(A.T, A) + n * np.eye(n) # Make A symmetric positive-definite\n",
        "b = np.random.rand(n, 1)\n",
        "x0 = np.zeros((n, 1))\n",
        "\n",
        "#Cholesky\n",
        "start_time = time.time()\n",
        "x_cho = solve_cholesky(A, b)\n",
        "time_cho = time.time() - start_time\n",
        "\n",
        "#Gauss-seidel\n",
        "start_time = time.time()\n",
        "x_gs, iter_gs = gauss_seidel_iteration(A, b, x0)\n",
        "time_gs = time.time()- start_time\n",
        "\n",
        "# Gradient Descent\n",
        "start_time = time.time()\n",
        "x_gd, iter_gd = gradient_descent(A, b, x0)\n",
        "time_gd = time.time()- start_time\n",
        "\n",
        "# Conjugate Gradient\n",
        "start_time = time.time()\n",
        "x_cg, iter_cg = conjugate_gradient(A, b, x0)\n",
        "time_cg = time.time()- start_time\n",
        "\n",
        "print(\"\\nCholesky:\")\n",
        "print(f\"Time: {time_cho:.6f} seconds\\n\")\n",
        "print(\"Gauss-Seidel:\")\n",
        "print(f\"Iterations: {iter_gs}, Time: {time_gs:.6f} seconds\\n\")\n",
        "print(\"Gradient Descent:\")\n",
        "print(f\"Iterations: {iter_gd}, Time: {time_gd:.6f} seconds\\n\")\n",
        "print(\"Conjugate Gradient:\")\n",
        "print(f\"Iterations: {iter_cg}, Time: {time_cg:.6f} seconds\\n\")\n",
        "print(np.linalg.norm(x_cho - x_gs))\n",
        "print(np.linalg.norm(x_gs - x_gd))\n",
        "print(np.linalg.norm(x_gd - x_cg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TFtmjP41iO8V",
        "outputId": "008fb200-42f6-46a1-d0ff-e44045dba398"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cholesky:\n",
            "Time: 0.002450 seconds\n",
            "\n",
            "Gauss-Seidel:\n",
            "Iterations: 20, Time: 0.002704 seconds\n",
            "\n",
            "Gradient Descent:\n",
            "Iterations: 26, Time: 0.000754 seconds\n",
            "\n",
            "Conjugate Gradient:\n",
            "Iterations: 1, Time: 0.000163 seconds\n",
            "\n",
            "0.28257446479407994\n",
            "4.5532613205476103e-11\n",
            "0.04078987506093969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-e3f2de999719>:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  y[i] = (b[i] - np.dot(L[i, :i], y[:i])) / L[i, i]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the case where $n=100$"
      ],
      "metadata": {
        "id": "eHig_pKahxl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "n = 100\n",
        "A = np.random.rand(n, n)\n",
        "A = np.matmul(A.T, A) + n * np.eye(n) # Make A symmetric positive-definite\n",
        "b = np.random.rand(n, 1)\n",
        "x0 = np.zeros((n, 1))\n",
        "\n",
        "#Cholesky\n",
        "start_time = time.time()\n",
        "x_cho = solve_cholesky(A, b)\n",
        "time_cho = time.time() - start_time\n",
        "\n",
        "#Gauss-seidel\n",
        "start_time = time.time()\n",
        "x_gs, iter_gs = gauss_seidel_iteration(A, b, x0)\n",
        "time_gs = time.time()- start_time\n",
        "\n",
        "# Gradient Descent\n",
        "start_time = time.time()\n",
        "x_gd, iter_gd = gradient_descent(A, b, x0)\n",
        "time_gd = time.time()- start_time\n",
        "\n",
        "# Conjugate Gradient\n",
        "start_time = time.time()\n",
        "x_cg, iter_cg = conjugate_gradient(A, b, x0)\n",
        "time_cg = time.time()- start_time\n",
        "\n",
        "print(\"\\nCholesky:\")\n",
        "print(f\"Time: {time_cho:.6f} seconds\\n\")\n",
        "print(\"Gauss-Seidel:\")\n",
        "print(f\"Iterations: {iter_gs}, Time: {time_gs:.6f} seconds\\n\")\n",
        "print(\"Gradient Descent:\")\n",
        "print(f\"Iterations: {iter_gd}, Time: {time_gd:.6f} seconds\\n\")\n",
        "print(\"Conjugate Gradient:\")\n",
        "print(f\"Iterations: {iter_cg}, Time: {time_cg:.6f} seconds\\n\")\n",
        "print(np.linalg.norm(x_cho - x_gs))\n",
        "print(np.linalg.norm(x_gs - x_gd))\n",
        "print(np.linalg.norm(x_gd - x_cg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6EVhhI3PgtF6",
        "outputId": "108e6874-a070-4812-f2d5-963e52b26bee"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-e3f2de999719>:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  y[i] = (b[i] - np.dot(L[i, :i], y[:i])) / L[i, i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cholesky:\n",
            "Time: 0.179223 seconds\n",
            "\n",
            "Gauss-Seidel:\n",
            "Iterations: 355, Time: 0.226717 seconds\n",
            "\n",
            "Gradient Descent:\n",
            "Iterations: 245, Time: 0.007392 seconds\n",
            "\n",
            "Conjugate Gradient:\n",
            "Iterations: 1, Time: 0.000222 seconds\n",
            "\n",
            "0.38233768041986693\n",
            "2.3340822296804838e-09\n",
            "0.02561077462688726\n"
          ]
        }
      ]
    }
  ]
}